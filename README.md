# Steps to Run
docker-compose up -d
If need to debug or test quickly, run parts outside Docker temporarily, but use Docker for integration

- In your terminal
docker-compose up -d
docker exec -it yrtsa-spark-1 bash

- Inside the Spark container
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:4.0.0 /app/kafka-spark.py (copy, right-click)

- In terminal 
python producer.py

# Real-Time-Sentiment-Analysis

YouTube API â†’ Kafka Producer â†’ Kafka Topic â†’ Spark Streaming Consumer â†’ NLP Sentiment Model â†’ Elasticsearch/Druid â†’ Dashboard (e.g., Kibana or Superset)

## Step by step

1. Extract YouTube comments using the YouTube Data API v3.
   - try extract 100k then tya Dr. how many records ?
  
ğŸ” STREAMING Workflow (Real-Time):
If you're handling live or continuous data, like live Twitter/YouTube/Kafka stream:

âœ… Recommended Order:
Extract (from API)
â†’ e.g., via Tweepy (Twitter) or Kafka Producer (YouTube comments stream)

Load into Kafka Topic
â†’ Raw, possibly noisy text is streamed into Kafka
â†’ Create kafka topic 

Kafka Consumer with PySpark Streaming
â†’ Read data in real-time from Kafka

Clean using PySpark Streaming logic
â†’ Tokenization, stopword removal, etc., all in-stream

Apply ML Model
â†’ Sentiment classification using pre-trained model (e.g., Hugging Face or scikit-learn)

Send to Elasticsearch
â†’ Store results for analytics and visualization in Kibana/Superset


youtube-sentiment-pipeline/ <br>
â”‚ <br>
â”œâ”€â”€ docker-compose.yml <br>
â”œâ”€â”€ kafka/ <br>
â”‚   â””â”€â”€ producer.py  # YouTube API to Kafka <br>
â”‚ <br>
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ spark_job.py  # NLP + ML processing
â”‚
â”œâ”€â”€ elastic/
â”‚   â””â”€â”€ elastic_setup.sh  # Optional Elasticsearch init
â”‚
â”œâ”€â”€ model/
â”‚   â””â”€â”€ sentiment_model.pkl  # Trained model (or use Hugging Face live)
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ kibana_config/
â”‚
â””â”€â”€ requirements.txt

